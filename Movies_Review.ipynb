{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example from training data: ['s', 'pretty', 'good', 'cast', 'film', 'has', 'nowhere', 'near', 'grace', 'original', 'italian', 'comedy', 'big', 'deal', 'madonna', 'street', 'anyone', 'looking', 'entertaining', 'caper', 'film', 'should', 'visit', 'original', 'william', 'macy', 'may', 'one', 'our', 'greatest', 'living', 'actors', 'here', 'he', 's', 'put', 'little', 'use', 'his', 'role', 'original', 'played', 'marcello', 'mastroianni', 'i', 'sort', 'feel', 'sorry', 'him', 'trying', 'fill', 'those', 'shoes', 'might', 'well', 'try', 'imitate', 'bogart', 'young', 'de', 'niro', 'art', 'direction', 'rich', 'textured', 'brings', 'nothing', 'story', 'extra', 'bits', 'add', 'story', 'feel', 'completely', 'unnecessary', 'things', 'take', 'away', 'missed', 'even', 'starting', 'way', 'do', 'seems', 'bizarrely', 'gratuitous', 'takes', 'away', 'surprise', 'original', 'sam', 'rockwell', 'has', 'his', 'odd', 'genial', 'charm', 'luis', 'guzman', 'has', 'odd', 'charisma', 'love', 'story', 'part', 'movie', 'seems', 'clunky', 'flat', 's', 'bad', 'nobody', 'has', 'figured', 'out', 'how', 'make', 'movie', 'well', 'first', 'made', 'then', 'again', 's', 'bad', 'live', 'culture', 'where', 'feel', 'like', 'need', 'remake', 'amazing', 'things', 'instead', 'simply', 'learning', 'savor', 'originals', 's_pretty', 'pretty_good', 'good_cast', 'cast_film', 'film_has', 'has_nowhere', 'nowhere_near', 'near_grace', 'grace_original', 'original_italian', 'italian_comedy', 'comedy_big', 'big_deal', 'deal_madonna', 'madonna_street', 'street_anyone', 'anyone_looking', 'looking_entertaining', 'entertaining_caper', 'caper_film', 'film_should', 'should_visit', 'visit_original', 'original_william', 'william_macy', 'macy_may', 'may_one', 'one_our', 'our_greatest', 'greatest_living', 'living_actors', 'actors_here', 'here_he', 'he_s', 's_put', 'put_little', 'little_use', 'use_his', 'his_role', 'role_original', 'original_played', 'played_marcello', 'marcello_mastroianni', 'mastroianni_i', 'i_sort', 'sort_feel', 'feel_sorry', 'sorry_him', 'him_trying', 'trying_fill', 'fill_those', 'those_shoes', 'shoes_might', 'might_well', 'well_try', 'try_imitate', 'imitate_bogart', 'bogart_young', 'young_de', 'de_niro', 'niro_art', 'art_direction', 'direction_rich', 'rich_textured', 'textured_brings', 'brings_nothing', 'nothing_story', 'story_extra', 'extra_bits', 'bits_add', 'add_story', 'story_feel', 'feel_completely', 'completely_unnecessary', 'unnecessary_things', 'things_take', 'take_away', 'away_missed', 'missed_even', 'even_starting', 'starting_way', 'way_do', 'do_seems', 'seems_bizarrely', 'bizarrely_gratuitous', 'gratuitous_takes', 'takes_away', 'away_surprise', 'surprise_original', 'original_sam', 'sam_rockwell', 'rockwell_has', 'has_his', 'his_odd', 'odd_genial', 'genial_charm', 'charm_luis', 'luis_guzman', 'guzman_has', 'has_odd', 'odd_charisma', 'charisma_love', 'love_story', 'story_part', 'part_movie', 'movie_seems', 'seems_clunky', 'clunky_flat', 'flat_s', 's_bad', 'bad_nobody', 'nobody_has', 'has_figured', 'figured_out', 'out_how', 'how_make', 'make_movie', 'movie_well', 'well_first', 'first_made', 'made_then', 'then_again', 'again_s', 's_bad', 'bad_live', 'live_culture', 'culture_where', 'where_feel', 'feel_like', 'like_need', 'need_remake', 'remake_amazing', 'amazing_things', 'things_instead', 'instead_simply', 'simply_learning', 'learning_savor', 'savor_originals', 's_pretty_good', 'pretty_good_cast', 'good_cast_film', 'cast_film_has', 'film_has_nowhere', 'has_nowhere_near', 'nowhere_near_grace', 'near_grace_original', 'grace_original_italian', 'original_italian_comedy', 'italian_comedy_big', 'comedy_big_deal', 'big_deal_madonna', 'deal_madonna_street', 'madonna_street_anyone', 'street_anyone_looking', 'anyone_looking_entertaining', 'looking_entertaining_caper', 'entertaining_caper_film', 'caper_film_should', 'film_should_visit', 'should_visit_original', 'visit_original_william', 'original_william_macy', 'william_macy_may', 'macy_may_one', 'may_one_our', 'one_our_greatest', 'our_greatest_living', 'greatest_living_actors', 'living_actors_here', 'actors_here_he', 'here_he_s', 'he_s_put', 's_put_little', 'put_little_use', 'little_use_his', 'use_his_role', 'his_role_original', 'role_original_played', 'original_played_marcello', 'played_marcello_mastroianni', 'marcello_mastroianni_i', 'mastroianni_i_sort', 'i_sort_feel', 'sort_feel_sorry', 'feel_sorry_him', 'sorry_him_trying', 'him_trying_fill', 'trying_fill_those', 'fill_those_shoes', 'those_shoes_might', 'shoes_might_well', 'might_well_try', 'well_try_imitate', 'try_imitate_bogart', 'imitate_bogart_young', 'bogart_young_de', 'young_de_niro', 'de_niro_art', 'niro_art_direction', 'art_direction_rich', 'direction_rich_textured', 'rich_textured_brings', 'textured_brings_nothing', 'brings_nothing_story', 'nothing_story_extra', 'story_extra_bits', 'extra_bits_add', 'bits_add_story', 'add_story_feel', 'story_feel_completely', 'feel_completely_unnecessary', 'completely_unnecessary_things', 'unnecessary_things_take', 'things_take_away', 'take_away_missed', 'away_missed_even', 'missed_even_starting', 'even_starting_way', 'starting_way_do', 'way_do_seems', 'do_seems_bizarrely', 'seems_bizarrely_gratuitous', 'bizarrely_gratuitous_takes', 'gratuitous_takes_away', 'takes_away_surprise', 'away_surprise_original', 'surprise_original_sam', 'original_sam_rockwell', 'sam_rockwell_has', 'rockwell_has_his', 'has_his_odd', 'his_odd_genial', 'odd_genial_charm', 'genial_charm_luis', 'charm_luis_guzman', 'luis_guzman_has', 'guzman_has_odd', 'has_odd_charisma', 'odd_charisma_love', 'charisma_love_story', 'love_story_part', 'story_part_movie', 'part_movie_seems', 'movie_seems_clunky', 'seems_clunky_flat', 'clunky_flat_s', 'flat_s_bad', 's_bad_nobody', 'bad_nobody_has', 'nobody_has_figured', 'has_figured_out', 'figured_out_how', 'out_how_make', 'how_make_movie', 'make_movie_well', 'movie_well_first', 'well_first_made', 'first_made_then', 'made_then_again', 'then_again_s', 'again_s_bad', 's_bad_live', 'bad_live_culture', 'live_culture_where', 'culture_where_feel', 'where_feel_like', 'feel_like_need', 'like_need_remake', 'need_remake_amazing', 'remake_amazing_things', 'amazing_things_instead', 'things_instead_simply', 'instead_simply_learning', 'simply_learning_savor', 'learning_savor_originals']\n",
      "Example from testing data: ['all', 'directors', 'ever', 'sit', 'behind', 'camera', 'wellman', 'could', 'break', 'heart', 'quicker', 'than', 'anyone', 'even', 'ford', 'one', 'his', 'worst', 'even', 'he', 'seemed', 'know', 'probably', 'its', 'jejune', 'treatment', 'not_much', 'above', 'bonanza', 'big', 'valley', 'yet', 'there', 'one', 'moment', 'typical', 'wellman', 'comes', 'out', 'nowhere', 'shatter', 'death', 'indian', 'wife', 'gable', 'she', 'has', 'gone', 'give', 'her', 'baby', 'some', 'water', 'wham', 'killed', 'arrow', 'instantly', 'no', 'warning', 'nada', 'one', 'most', 'shocking', 'unprepared', 'deaths', 'all', 'cinema', 'particularly', 'after', 'investing', 'whole', 'hour', 'love', 'lady', 'what', 'looked', 'sappy', 'western', 'course', 'treatment', 'her', 'afterwards', 'laden', 'wellman', 's', 'total', 'indifference', 'apparently', 'film', 'even', 'reminder', 'power', 'he', 'has', 'always', 'had', 'his', 'best', 'all_directors', 'directors_ever', 'ever_sit', 'sit_behind', 'behind_camera', 'camera_wellman', 'wellman_could', 'could_break', 'break_heart', 'heart_quicker', 'quicker_than', 'than_anyone', 'anyone_even', 'even_ford', 'ford_one', 'one_his', 'his_worst', 'worst_even', 'even_he', 'he_seemed', 'seemed_know', 'know_probably', 'probably_its', 'its_jejune', 'jejune_treatment', 'treatment_not_much', 'not_much_above', 'above_bonanza', 'bonanza_big', 'big_valley', 'valley_yet', 'yet_there', 'there_one', 'one_moment', 'moment_typical', 'typical_wellman', 'wellman_comes', 'comes_out', 'out_nowhere', 'nowhere_shatter', 'shatter_death', 'death_indian', 'indian_wife', 'wife_gable', 'gable_she', 'she_has', 'has_gone', 'gone_give', 'give_her', 'her_baby', 'baby_some', 'some_water', 'water_wham', 'wham_killed', 'killed_arrow', 'arrow_instantly', 'instantly_no', 'no_warning', 'warning_nada', 'nada_one', 'one_most', 'most_shocking', 'shocking_unprepared', 'unprepared_deaths', 'deaths_all', 'all_cinema', 'cinema_particularly', 'particularly_after', 'after_investing', 'investing_whole', 'whole_hour', 'hour_love', 'love_lady', 'lady_what', 'what_looked', 'looked_sappy', 'sappy_western', 'western_course', 'course_treatment', 'treatment_her', 'her_afterwards', 'afterwards_laden', 'laden_wellman', 'wellman_s', 's_total', 'total_indifference', 'indifference_apparently', 'apparently_film', 'film_even', 'even_reminder', 'reminder_power', 'power_he', 'he_has', 'has_always', 'always_had', 'had_his', 'his_best', 'all_directors_ever', 'directors_ever_sit', 'ever_sit_behind', 'sit_behind_camera', 'behind_camera_wellman', 'camera_wellman_could', 'wellman_could_break', 'could_break_heart', 'break_heart_quicker', 'heart_quicker_than', 'quicker_than_anyone', 'than_anyone_even', 'anyone_even_ford', 'even_ford_one', 'ford_one_his', 'one_his_worst', 'his_worst_even', 'worst_even_he', 'even_he_seemed', 'he_seemed_know', 'seemed_know_probably', 'know_probably_its', 'probably_its_jejune', 'its_jejune_treatment', 'jejune_treatment_not_much', 'treatment_not_much_above', 'not_much_above_bonanza', 'above_bonanza_big', 'bonanza_big_valley', 'big_valley_yet', 'valley_yet_there', 'yet_there_one', 'there_one_moment', 'one_moment_typical', 'moment_typical_wellman', 'typical_wellman_comes', 'wellman_comes_out', 'comes_out_nowhere', 'out_nowhere_shatter', 'nowhere_shatter_death', 'shatter_death_indian', 'death_indian_wife', 'indian_wife_gable', 'wife_gable_she', 'gable_she_has', 'she_has_gone', 'has_gone_give', 'gone_give_her', 'give_her_baby', 'her_baby_some', 'baby_some_water', 'some_water_wham', 'water_wham_killed', 'wham_killed_arrow', 'killed_arrow_instantly', 'arrow_instantly_no', 'instantly_no_warning', 'no_warning_nada', 'warning_nada_one', 'nada_one_most', 'one_most_shocking', 'most_shocking_unprepared', 'shocking_unprepared_deaths', 'unprepared_deaths_all', 'deaths_all_cinema', 'all_cinema_particularly', 'cinema_particularly_after', 'particularly_after_investing', 'after_investing_whole', 'investing_whole_hour', 'whole_hour_love', 'hour_love_lady', 'love_lady_what', 'lady_what_looked', 'what_looked_sappy', 'looked_sappy_western', 'sappy_western_course', 'western_course_treatment', 'course_treatment_her', 'treatment_her_afterwards', 'her_afterwards_laden', 'afterwards_laden_wellman', 'laden_wellman_s', 'wellman_s_total', 's_total_indifference', 'total_indifference_apparently', 'indifference_apparently_film', 'apparently_film_even', 'film_even_reminder', 'even_reminder_power', 'reminder_power_he', 'power_he_has', 'he_has_always', 'has_always_had', 'always_had_his', 'had_his_best']\n",
      "Number of words in vocabulary: 5251541\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenized data from files\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "import math\n",
    "from collections import Counter\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "\n",
    "data_folder = \"C:/Users/Sanika Salunke/Desktop/Applied_ML_Assigmnt/Tokenizer_Result\"\n",
    "\n",
    "train_data = pickle.load(open(os.path.join(data_folder, \"train_data.pkl\"), \"rb\"))\n",
    "test_data = pickle.load(open(os.path.join(data_folder, \"test_data.pkl\"), \"rb\"))\n",
    "train_labels = pickle.load(open(os.path.join(data_folder, \"train_labels.pkl\"), \"rb\"))\n",
    "test_labels = pickle.load(open(os.path.join(data_folder, \"test_labels.pkl\"), \"rb\"))\n",
    "vocab = pickle.load(open(os.path.join(data_folder, \"vocab.pkl\"), \"rb\"))\n",
    "\n",
    "\n",
    "print(\"Example from training data:\", train_data[0] if train_data else \"No training data found\")\n",
    "print(\"Example from testing data:\", test_data[0] if test_data else \"No testing data found\")\n",
    "print(\"Number of words in vocabulary:\", len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data (bigrams) sample: ['s_pretty', 'pretty_good', 'good_cast', 'cast_film', 'film_has', 'has_nowhere', 'nowhere_near', 'near_grace', 'grace_original', 'original_italian', 'italian_comedy', 'comedy_big', 'big_deal', 'deal_madonna', 'madonna_street', 'street_anyone', 'anyone_looking', 'looking_entertaining', 'entertaining_caper', 'caper_film', 'film_should', 'should_visit', 'visit_original', 'original_william', 'william_macy', 'macy_may', 'may_one', 'one_our', 'our_greatest', 'greatest_living', 'living_actors', 'actors_here', 'here_he', 'he_s', 's_put', 'put_little', 'little_use', 'use_his', 'his_role', 'role_original', 'original_played', 'played_marcello', 'marcello_mastroianni', 'mastroianni_i', 'i_sort', 'sort_feel', 'feel_sorry', 'sorry_him', 'him_trying', 'trying_fill', 'fill_those', 'those_shoes', 'shoes_might', 'might_well', 'well_try', 'try_imitate', 'imitate_bogart', 'bogart_young', 'young_de', 'de_niro', 'niro_art', 'art_direction', 'direction_rich', 'rich_textured', 'textured_brings', 'brings_nothing', 'nothing_story', 'story_extra', 'extra_bits', 'bits_add', 'add_story', 'story_feel', 'feel_completely', 'completely_unnecessary', 'unnecessary_things', 'things_take', 'take_away', 'away_missed', 'missed_even', 'even_starting', 'starting_way', 'way_do', 'do_seems', 'seems_bizarrely', 'bizarrely_gratuitous', 'gratuitous_takes', 'takes_away', 'away_surprise', 'surprise_original', 'original_sam', 'sam_rockwell', 'rockwell_has', 'has_his', 'his_odd', 'odd_genial', 'genial_charm', 'charm_luis', 'luis_guzman', 'guzman_has', 'has_odd', 'odd_charisma', 'charisma_love', 'love_story', 'story_part', 'part_movie', 'movie_seems', 'seems_clunky', 'clunky_flat', 'flat_s', 's_bad', 'bad_nobody', 'nobody_has', 'has_figured', 'figured_out', 'out_how', 'how_make', 'make_movie', 'movie_well', 'well_first', 'first_made', 'made_then', 'then_again', 'again_s', 's_bad', 'bad_live', 'live_culture', 'culture_where', 'where_feel', 'feel_like', 'like_need', 'need_remake', 'remake_amazing', 'amazing_things', 'things_instead', 'instead_simply', 'simply_learning', 'learning_savor', 'savor_originals']\n",
      "Test data (bigrams) sample: ['all_directors', 'directors_ever', 'ever_sit', 'sit_behind', 'behind_camera', 'camera_wellman', 'wellman_could', 'could_break', 'break_heart', 'heart_quicker', 'quicker_than', 'than_anyone', 'anyone_even', 'even_ford', 'ford_one', 'one_his', 'his_worst', 'worst_even', 'even_he', 'he_seemed', 'seemed_know', 'know_probably', 'probably_its', 'its_jejune', 'jejune_treatment', 'treatment_not_much', 'not_much_above', 'above_bonanza', 'bonanza_big', 'big_valley', 'valley_yet', 'yet_there', 'there_one', 'one_moment', 'moment_typical', 'typical_wellman', 'wellman_comes', 'comes_out', 'out_nowhere', 'nowhere_shatter', 'shatter_death', 'death_indian', 'indian_wife', 'wife_gable', 'gable_she', 'she_has', 'has_gone', 'gone_give', 'give_her', 'her_baby', 'baby_some', 'some_water', 'water_wham', 'wham_killed', 'killed_arrow', 'arrow_instantly', 'instantly_no', 'no_warning', 'warning_nada', 'nada_one', 'one_most', 'most_shocking', 'shocking_unprepared', 'unprepared_deaths', 'deaths_all', 'all_cinema', 'cinema_particularly', 'particularly_after', 'after_investing', 'investing_whole', 'whole_hour', 'hour_love', 'love_lady', 'lady_what', 'what_looked', 'looked_sappy', 'sappy_western', 'western_course', 'course_treatment', 'treatment_her', 'her_afterwards', 'afterwards_laden', 'laden_wellman', 'wellman_s', 's_total', 'total_indifference', 'indifference_apparently', 'apparently_film', 'film_even', 'even_reminder', 'reminder_power', 'power_he', 'he_has', 'has_always', 'always_had', 'had_his', 'his_best']\n",
      "Bigram vocabulary size: 1805104\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# loading bigram only data\n",
    "train_data_bigrams = pickle.load(open(os.path.join(data_folder, \"train_data_bigrams.pkl\"), \"rb\"))\n",
    "test_data_bigrams = pickle.load(open(os.path.join(data_folder, \"test_data_bigrams.pkl\"), \"rb\"))\n",
    "vocab_bigrams = pickle.load(open(os.path.join(data_folder, \"vocab_bigrams.pkl\"), \"rb\"))\n",
    "\n",
    "\n",
    "print(\"Train data (bigrams) sample:\", train_data_bigrams[0] if train_data_bigrams else \"No data found\")\n",
    "print(\"Test data (bigrams) sample:\", test_data_bigrams[0] if test_data_bigrams else \"No data found\")\n",
    "print(\"Bigram vocabulary size:\", len(vocab_bigrams))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Feature Extraction with TF-IDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before filtering token count: 5251541\n",
      "After filtering token count: 434\n",
      "First training review tf idf values:\n",
      "{'s': 0.06140163146933402, 'pretty': 0.03664876979641259, 'good': 0.022769708548521807, 'cast': 0.035427248049907954, 'film': 0.0368357728803567, 'has': 0.08883031465006824, 'original': 0.15666686677020691, 'comedy': 0.03910232343290857, 'big': 0.03705580844986601, 'anyone': 0.03921150646023228, 'looking': 0.04010779851785525, 'entertaining': 0.04578492284453531, 'should': 0.03267873086062223, 'may': 0.03756817123217647, 'one': 0.018329820990682647, 'our': 0.04187245497684678, 'actors': 0.03410458647785022, 'here': 0.03195140536344031, 'he': 0.02187166961903355, 'put': 0.040221453621690335, 'little': 0.030706537228766137, 'use': 0.043797292210014035, 'his': 0.043062028319731493, 'role': 0.03859962573167883, 'played': 0.0403692332000237, 'i': 0.014276676314789703, 'sort': 0.04623220517974224, 'feel': 0.11586990902762337, 'him': 0.030055643012136625, 'trying': 0.04021058133761002, 'those': 0.03361988906159315, 'might': 0.03870416830489037, 'well': 0.05124681124429372, 'try': 0.043319067891131914, 'young': 0.037439018771764795, 'direction': 0.04618669472251972, 'nothing': 0.03469797578032052, 'story': 0.07633044608266282, 'completely': 0.04349070063047052, 'things': 0.07257295172793837, 'take': 0.03654126912031898, 'away': 0.07759043540341572, 'even': 0.024865985837852357, 'way': 0.02815318933910023, 'do': 0.027088093331410497, 'seems': 0.0733375737117727, 'takes': 0.041363480355809855, 'love': 0.031658681508601856, 'part': 0.035667382322612906, 'movie': 0.03465916976151777, 'bad': 0.05720184991966808, 'out': 0.02158892708038375, 'how': 0.02762776823564846, 'make': 0.02816475364336195, 'first': 0.027428777295625133, 'made': 0.02757265956862641, 'then': 0.028911345517443468, 'again': 0.0353447166556123, 'live': 0.04572365409244215, 'where': 0.03088059646687888, 'like': 0.02057014004713136, 'need': 0.04391624881758165, 'instead': 0.04148410287915088, 'simply': 0.04312198000376246, 'he_s': 0.040710186406022156}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Additional feature selection  \n",
    "min_percent = 0.05\n",
    "total_docs = len(train_data)\n",
    "min_count = math.ceil(min_percent * total_docs)\n",
    "\n",
    "doc_count = Counter()\n",
    "for review in train_data:\n",
    "    doc_count.update(set(review))\n",
    "\n",
    "selected_tokens = {token for token, count in doc_count.items() if count >= min_count}\n",
    "print(\"Before filtering token count:\", len(doc_count))\n",
    "print(\"After filtering token count:\", len(selected_tokens))\n",
    "\n",
    "idf = {}\n",
    "for token in selected_tokens:\n",
    "    df = doc_count[token]\n",
    "    idf[token] = math.log((total_docs + 1) / (df + 1)) + 1\n",
    "\n",
    "# Manual Implementation of feature extraction using tf - idf values\n",
    "\n",
    "def calcTfidfval(review, selected_tokens, idf):\n",
    "    token_counts = Counter(token for token in review if token in selected_tokens)\n",
    "    tfidf = {}\n",
    "    total_tokens = sum(token_counts.values())\n",
    "    if total_tokens == 0:\n",
    "        return tfidf\n",
    "    for token, count in token_counts.items():\n",
    "        tf = count / total_tokens\n",
    "        tfidf[token] = tf * idf[token]\n",
    "    return tfidf\n",
    "\n",
    "trainedTfidval =  [calcTfidfval(review, selected_tokens, idf) for review in train_data]\n",
    "testTfidfval =  [calcTfidfval(review, selected_tokens, idf) for review in test_data]\n",
    "\n",
    "print(\"First training review tf idf values:\")\n",
    "print(trainedTfidval[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of bigrams before filtering: 1805104\n",
      "Number of bigrams after filtering: 30\n",
      "\n",
      "Sample TF-IDF features for the first training review (bigrams):\n",
      "{'he_s': 3.5010760309179054}\n"
     ]
    }
   ],
   "source": [
    "# Computing TF-IDF features for bigrams\n",
    "\n",
    "\n",
    "doc_freq_bigrams = Counter()\n",
    "for review in train_data_bigrams:\n",
    "    doc_freq_bigrams.update(set(review))\n",
    "\n",
    "\n",
    "selected_bigrams = {bigram for bigram, count in doc_freq_bigrams.items() if count >= min_count}\n",
    "print(\"Number of bigrams before filtering:\", len(doc_freq_bigrams))\n",
    "print(\"Number of bigrams after filtering:\", len(selected_bigrams))\n",
    "\n",
    "\n",
    "idf_bigrams = {}\n",
    "for bigram in selected_bigrams:\n",
    "    df = doc_freq_bigrams[bigram]\n",
    "    idf_bigrams[bigram] = math.log((total_docs + 1) / (df + 1)) + 1\n",
    "\n",
    "\n",
    "train_tfidf_bigrams = [calcTfidfval(review, selected_bigrams, idf_bigrams) for review in train_data_bigrams]\n",
    "test_tfidf_bigrams = [calcTfidfval(review, selected_bigrams, idf_bigrams) for review in test_data_bigrams]\n",
    "\n",
    "print(\"\\nSample TF-IDF features for the first training review (bigrams):\")\n",
    "print(train_tfidf_bigrams[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Bayesian Classification - Theoretical Justification  \n",
    "\n",
    "Bayesian Theorem is one of the most renowned and fundamental concepts in statistics. It is based on calculating the probabilities (posterior and prior) and is used to describe how we can modify our belief based on new situations. \n",
    "Mathematically it is expressed as:\n",
    "\tP(C∣X) =P(X)P(X∣C) P(C) \n",
    "where:\n",
    "•\tP(C∣X) P(C|X) P(C∣X) is the posterior probability, which is the probability of class C given the observed data X.\n",
    "•\tP(X∣C) P(X|C) P(X∣C) is the likelihood, which is the probability of observing X given that class C is true.\n",
    "•\tP(C)P(C)P(C) is the prior probability, which is our initial belief about class C before seeing any data.\n",
    "•\tP(X)P(X)P(X) is the marginal probability of the observed data X across all possible classes.\n",
    "The Naïve Bayes assumption states that the features (tokens w.r.t text classification) are conditionally independent given the class label. That is:\n",
    "P(X∣C)=P(x1,x2,...,xn∣ C)=P(x1∣C)P(x2∣C)...P(xn ∣C)P(X | C) = P(x_1, x_2, ..., x_n | C) = P(x_1 | C) P(x_2 | C) ... P(x_n | C)P(X∣C)=P(x1,x2,...,xn∣C)=P(x1∣C)P(x2∣C)...P(xn∣C) \n",
    "\n",
    "But this assumption helps to simplify the computation making naïve bayes a better model for high dimensional data. Still, it is unrealistic since words in text data are contextually related.\n",
    "\n",
    "1.\tPrior Probability:\n",
    "P(C)=count of documents in class C / total number of documents \n",
    "2.\tLikelihood with Laplace Smoothing:\n",
    "Since some tokens may not appear in each class, Laplace smoothing ensures that probabilities never become zero. The likelihood of a word w in class C is:\n",
    "P(w∣C) = count of w in documents of class C+1 / total word count in class C+∣V∣ \n",
    "where ∣V∣ is the vocabulary size.\n",
    "3.\tFinal Classification Formula:\n",
    "Using Bayes’ rule and the Naïve Bayes assumption, we classify a new document based on:\n",
    " \n",
    "To avoid numerical underflow, we take the logarithm of probabilities:\n",
    "\t\t \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Implementing Bayesian Classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Train Accuracy: 0.80184\n",
      "Baseline Test Accuracy: 0.8003520140805632\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "def train_bayes_classifier(data_tfidf, data_labels):\n",
    "    #Getting all unique classes\n",
    "    classes = set(data_labels)\n",
    "    class_counts = Counter(data_labels)\n",
    "    total_labels = len(data_labels)\n",
    "    \n",
    "  # Calculating prior prob for each classes\n",
    "    priors = {c: class_counts[c] / total_labels for c in classes}\n",
    "    \n",
    "    #initializing token sums and total weights for each classes\n",
    "    token_sums = {c: defaultdict(float) for c in classes}\n",
    "    total_weights = {c: 0.0 for c in classes}\n",
    "    \n",
    "    # Summing up the tfidf weights for each token in each class\n",
    "    for tfidf, label in zip(data_tfidf, data_labels):\n",
    "        for token, weight in tfidf.items():\n",
    "            token_sums[label][token] += weight\n",
    "            total_weights[label] += weight\n",
    "    \n",
    "    # Calculating token probabilities for each class\n",
    "    vocab_size = len(selected_tokens)\n",
    "    # Laplace Smoothing to avoid zero prob scenarios\n",
    "    token_probs = {c: {} for c in classes}\n",
    "    for c in classes:\n",
    "        for token in selected_tokens:\n",
    "            token_sum = token_sums[c].get(token, 0.0)\n",
    "            token_probs[c][token] = (token_sum + 1) / (total_weights[c] + vocab_size)\n",
    "    \n",
    "   \n",
    "    def predict(review_tfidf):\n",
    "        scores = {}\n",
    "        #Using logarithms for numerical stability\n",
    "        for c in classes:\n",
    "            score = math.log(priors[c])  \n",
    "            for token, weight in review_tfidf.items():\n",
    "                if token in selected_tokens:\n",
    "                    score += weight * math.log(token_probs[c][token])\n",
    "                else:\n",
    "                    score += weight * math.log(1 / (total_weights[c] + vocab_size))\n",
    "            scores[c] = score\n",
    "        return max(scores, key=scores.get) \n",
    "    \n",
    "    return predict\n",
    "\n",
    "# Training the classifier\n",
    "clf_baseline = train_bayes_classifier(trainedTfidval, train_labels)\n",
    "\n",
    "# Making predictions for training and testing data\n",
    "train_preds = [clf_baseline(x) for x in trainedTfidval]\n",
    "test_preds = [clf_baseline(x) for x in testTfidfval]\n",
    "\n",
    "# calculating accuracy\n",
    "train_acc = sum(1 for p, t in zip(train_preds, train_labels) if p == t) / len(train_labels)\n",
    "test_acc = sum(1 for p, t in zip(test_preds, test_labels) if p == t) / len(test_labels)\n",
    "\n",
    "print(\"Baseline Train Accuracy:\", train_acc)\n",
    "print(\"Baseline Test Accuracy:\", test_acc)\n",
    "\n",
    "\n",
    "# Count TP, FP, and FN for the \"positive\" class\n",
    "tp = sum(1 for p, t in zip(test_preds, test_labels) if p == \"positive\" and t == \"positive\")\n",
    "fp = sum(1 for p, t in zip(test_preds, test_labels) if p == \"positive\" and t == \"negative\")\n",
    "fn = sum(1 for p, t in zip(test_preds, test_labels) if p == \"negative\" and t == \"positive\")\n",
    "\n",
    "# Calculate precision and recall\n",
    "precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "recall = tp / (tp + fn) if (tp + fn) > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram Model Train Accuracy: 0.58724\n",
      "Bigram Model Test Accuracy: 0.5849033961358454\n"
     ]
    }
   ],
   "source": [
    "# Training the classifier for bigram data\n",
    "\n",
    "clf_bigrams = train_bayes_classifier(train_tfidf_bigrams, train_labels)\n",
    "\n",
    "# making predictions for bigram only model\n",
    "train_preds_bigrams = [clf_bigrams(x) for x in train_tfidf_bigrams]\n",
    "test_preds_bigrams = [clf_bigrams(x) for x in test_tfidf_bigrams]\n",
    "\n",
    "# calculating accuracy for bigram only model\n",
    "train_acc_bigrams = sum(1 for p, t in zip(train_preds_bigrams, train_labels) if p == t) / len(train_labels)\n",
    "test_acc_bigrams = sum(1 for p, t in zip(test_preds_bigrams, test_labels) if p == t) / len(test_labels)\n",
    "\n",
    "print(\"Bigram Model Train Accuracy:\", train_acc_bigrams)\n",
    "print(\"Bigram Model Test Accuracy:\", test_acc_bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Handling Class Imbalances & Evaluating Performance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-Fold CV Average Accuracy: 0.79928\n",
      "Confusion Matrix: {'negative': {'negative': 10081, 'positive': 2419}, 'positive': {'positive': 9927, 'negative': 2572}}\n",
      "Test Accuracy: 0.8003520140805632\n",
      "Precision: {'positive': 0.8040660942815486, 'negative': 0.7967280486841065}\n",
      "Recall: {'positive': 0.7942235378830307, 'negative': 0.80648}\n",
      "F1-Score: {'positive': 0.7991145099617629, 'negative': 0.8015743648868922}\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Since the model 1 which is trained on all 3 types ok tokens is performing better than the bigram model, that model is used for further analysis\n",
    "\n",
    "\n",
    "\n",
    "def undersample(data_tfidf, data_labels):\n",
    "    counts = Counter(data_labels)\n",
    "    min_count = min(counts.values())\n",
    "    indices_by_class = {label: [] for label in counts}\n",
    "    for i, label in enumerate(data_labels):\n",
    "        indices_by_class[label].append(i)\n",
    "    new_indices = []\n",
    "    for label in counts:\n",
    "        new_indices.extend(random.sample(indices_by_class[label], min_count))\n",
    "    new_indices.sort()\n",
    "    return [data_tfidf[i] for i in new_indices], [data_labels[i] for i in new_indices]\n",
    "\n",
    "# K - fold cross validation using 5 folds\n",
    "\n",
    "k = 5\n",
    "indices = list(range(len(trainedTfidval)))\n",
    "random.shuffle(indices)\n",
    "fold_size = len(indices) // k\n",
    "fold_accuracies = []\n",
    "\n",
    "for i in range(k):\n",
    "    val_indices = indices[i * fold_size: (i + 1) * fold_size] if i < k - 1 else indices[i * fold_size:]\n",
    "    train_indices = [j for j in indices if j not in val_indices]\n",
    "    fold_trainedTfidval = [trainedTfidval[j] for j in train_indices]\n",
    "    fold_train_labels = [train_labels[j] for j in train_indices]\n",
    "    fold_trainedTfidval_us, fold_train_labels_us = undersample(fold_trainedTfidval, fold_train_labels)\n",
    "    clf = train_bayes_classifier(fold_trainedTfidval_us, fold_train_labels_us)\n",
    "    val_preds = [clf(trainedTfidval[j]) for j in val_indices]\n",
    "    fold_true = [train_labels[j] for j in val_indices]\n",
    "    acc = sum(1 for p, t in zip(val_preds, fold_true) if p == t) / len(val_preds)\n",
    "    fold_accuracies.append(acc)\n",
    "\n",
    "print(\"5-Fold CV Average Accuracy:\", sum(fold_accuracies) / len(fold_accuracies))\n",
    "\n",
    "# Under Sampling \n",
    "trainedTfidval_us_full, train_labels_us_full = undersample(trainedTfidval, train_labels)\n",
    "clf_final = train_bayes_classifier(trainedTfidval_us_full, train_labels_us_full)\n",
    "test_preds_final = [clf_final(x) for x in testTfidfval]\n",
    "\n",
    "# Confusion Matrix\n",
    "confusion = {}\n",
    "for true, pred in zip(test_labels, test_preds_final):\n",
    "    confusion.setdefault(true, {})\n",
    "    confusion[true][pred] = confusion[true].get(pred, 0) + 1\n",
    "print(\"Confusion Matrix:\", confusion)\n",
    "\n",
    "# Precision, Recall, F1-Score, Accuracy\n",
    "precision = {}\n",
    "recall = {}\n",
    "f1 = {}\n",
    "all_classes = set(train_labels)\n",
    "for c in all_classes:\n",
    "    tp = confusion.get(c, {}).get(c, 0)\n",
    "    fp = sum(confusion.get(other, {}).get(c, 0) for other in all_classes if other != c)\n",
    "    fn = sum(confusion.get(c, {}).get(other, 0) for other in all_classes if other != c)\n",
    "    precision[c] = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall[c] = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1[c] = 2 * precision[c] * recall[c] / (precision[c] + recall[c]) if (precision[c] + recall[c]) > 0 else 0\n",
    "\n",
    "accuracy_final = sum(1 for p, t in zip(test_preds_final, test_labels) if p == t) / len(test_labels)\n",
    "print(\"Test Accuracy:\", accuracy_final)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-Score:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6: Analysis of Errors & Manual Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All done! Processed data saved in Tokenizer_Result.\n",
      "Bigram data saved too. You're good to go!\n",
      "5 Misclassified Reviews (index, predicted, true, review):\n",
      "Index: 1 | Predicted: positive | True: negative | Review: do not_take film seriously rent some folks who want play mystery science 3000 will probably laugh butts off evil guys not_scary s funny s like some dude 7th grade sickle scarecrow get up acting hilarious i love occasional self torture poor horror film really had me giggling i recommend basis course recreational drugs will enhance experience oh there naked group swimming scene will allow some star dust 5 star system token black male gets injured badly continues his joking well using injured body part quite readily throughout enjoy complete utter disgrace films do_not_take not_take_film film_seriously seriously_rent rent_some some_folks folks_who who_want want_play play_mystery mystery_science science_3000 3000_will will_probably probably_laugh laugh_butts butts_off off_evil evil_guys guys_not_scary not_scary_s s_funny funny_s s_like like_some some_dude dude_7th 7th_grade grade_sickle sickle_scarecrow scarecrow_get get_up up_acting acting_hilarious hilarious_i i_love love_occasional occasional_self self_torture torture_poor poor_horror horror_film film_really really_had had_me me_giggling giggling_i i_recommend recommend_basis basis_course course_recreational recreational_drugs drugs_will will_enhance enhance_experience experience_oh oh_there there_naked naked_group group_swimming swimming_scene scene_will will_allow allow_some some_star star_dust dust_5 5_star star_system system_token token_black black_male male_gets gets_injured injured_badly badly_continues continues_his his_joking joking_well well_using using_injured injured_body body_part part_quite quite_readily readily_throughout throughout_enjoy enjoy_complete complete_utter utter_disgrace disgrace_films do_not_take_film not_take_film_seriously film_seriously_rent seriously_rent_some rent_some_folks some_folks_who folks_who_want who_want_play want_play_mystery play_mystery_science mystery_science_3000 science_3000_will 3000_will_probably will_probably_laugh probably_laugh_butts laugh_butts_off butts_off_evil off_evil_guys evil_guys_not_scary guys_not_scary_s not_scary_s_funny s_funny_s funny_s_like s_like_some like_some_dude some_dude_7th dude_7th_grade 7th_grade_sickle grade_sickle_scarecrow sickle_scarecrow_get scarecrow_get_up get_up_acting up_acting_hilarious acting_hilarious_i hilarious_i_love i_love_occasional love_occasional_self occasional_self_torture self_torture_poor torture_poor_horror poor_horror_film horror_film_really film_really_had really_had_me had_me_giggling me_giggling_i giggling_i_recommend i_recommend_basis recommend_basis_course basis_course_recreational course_recreational_drugs recreational_drugs_will drugs_will_enhance will_enhance_experience enhance_experience_oh experience_oh_there oh_there_naked there_naked_group naked_group_swimming group_swimming_scene swimming_scene_will scene_will_allow will_allow_some allow_some_star some_star_dust star_dust_5 dust_5_star 5_star_system star_system_token system_token_black token_black_male black_male_gets male_gets_injured gets_injured_badly injured_badly_continues badly_continues_his continues_his_joking his_joking_well joking_well_using well_using_injured using_injured_body injured_body_part body_part_quite part_quite_readily quite_readily_throughout readily_throughout_enjoy throughout_enjoy_complete enjoy_complete_utter complete_utter_disgrace utter_disgrace_films\n",
      "Index: 7 | Predicted: positive | True: negative | Review: kokoda inspired events kokoda track during ww2 when australian militia slowed ultimately stopped push 10 000 japanese soldiers move overland capture port moresby what really mean movie set time period fiction everything happens jumble standard scenes other war films first hour one cliché after another some scenes simply there able draw us into feeling conflict horrific beyond compare when there appears little evidence both sides fought hard control track no mercy shown either side both sides suffered logistic shortages terrain great leveler conflict japanese got closer port moresby supply line grew ultimately led downfall other hand australians retreated closer port moresby supply line decreased some scenes appear straight out handbook standard scenes include any war film film misguided highlighted youth production team time when australia could have done great film about one australia s best moments film kokoda shallow disappointment kokoda_inspired inspired_events events_kokoda kokoda_track track_during during_ww2 ww2_when when_australian australian_militia militia_slowed slowed_ultimately ultimately_stopped stopped_push push_10 10_000 000_japanese japanese_soldiers soldiers_move move_overland overland_capture capture_port port_moresby moresby_what what_really really_mean mean_movie movie_set set_time time_period period_fiction fiction_everything everything_happens happens_jumble jumble_standard standard_scenes scenes_other other_war war_films films_first first_hour hour_one one_cliché cliché_after after_another another_some some_scenes scenes_simply simply_there there_able able_draw draw_us us_into into_feeling feeling_conflict conflict_horrific horrific_beyond beyond_compare compare_when when_there there_appears appears_little little_evidence evidence_both both_sides sides_fought fought_hard hard_control control_track track_no no_mercy mercy_shown shown_either either_side side_both both_sides sides_suffered suffered_logistic logistic_shortages shortages_terrain terrain_great great_leveler leveler_conflict conflict_japanese japanese_got got_closer closer_port port_moresby moresby_supply supply_line line_grew grew_ultimately ultimately_led led_downfall downfall_other other_hand hand_australians australians_retreated retreated_closer closer_port port_moresby moresby_supply supply_line line_decreased decreased_some some_scenes scenes_appear appear_straight straight_out out_handbook handbook_standard standard_scenes scenes_include include_any any_war war_film film_film film_misguided misguided_highlighted highlighted_youth youth_production production_team team_time time_when when_australia australia_could could_have have_done done_great great_film film_about about_one one_australia australia_s s_best best_moments moments_film film_kokoda kokoda_shallow shallow_disappointment kokoda_inspired_events inspired_events_kokoda events_kokoda_track kokoda_track_during track_during_ww2 during_ww2_when ww2_when_australian when_australian_militia australian_militia_slowed militia_slowed_ultimately slowed_ultimately_stopped ultimately_stopped_push stopped_push_10 push_10_000 10_000_japanese 000_japanese_soldiers japanese_soldiers_move soldiers_move_overland move_overland_capture overland_capture_port capture_port_moresby port_moresby_what moresby_what_really what_really_mean really_mean_movie mean_movie_set movie_set_time set_time_period time_period_fiction period_fiction_everything fiction_everything_happens everything_happens_jumble happens_jumble_standard jumble_standard_scenes standard_scenes_other scenes_other_war other_war_films war_films_first films_first_hour first_hour_one hour_one_cliché one_cliché_after cliché_after_another after_another_some another_some_scenes some_scenes_simply scenes_simply_there simply_there_able there_able_draw able_draw_us draw_us_into us_into_feeling into_feeling_conflict feeling_conflict_horrific conflict_horrific_beyond horrific_beyond_compare beyond_compare_when compare_when_there when_there_appears there_appears_little appears_little_evidence little_evidence_both evidence_both_sides both_sides_fought sides_fought_hard fought_hard_control hard_control_track control_track_no track_no_mercy no_mercy_shown mercy_shown_either shown_either_side either_side_both side_both_sides both_sides_suffered sides_suffered_logistic suffered_logistic_shortages logistic_shortages_terrain shortages_terrain_great terrain_great_leveler great_leveler_conflict leveler_conflict_japanese conflict_japanese_got japanese_got_closer got_closer_port closer_port_moresby port_moresby_supply moresby_supply_line supply_line_grew line_grew_ultimately grew_ultimately_led ultimately_led_downfall led_downfall_other downfall_other_hand other_hand_australians hand_australians_retreated australians_retreated_closer retreated_closer_port closer_port_moresby port_moresby_supply moresby_supply_line supply_line_decreased line_decreased_some decreased_some_scenes some_scenes_appear scenes_appear_straight appear_straight_out straight_out_handbook out_handbook_standard handbook_standard_scenes standard_scenes_include scenes_include_any include_any_war any_war_film war_film_film film_film_misguided film_misguided_highlighted misguided_highlighted_youth highlighted_youth_production youth_production_team production_team_time team_time_when time_when_australia when_australia_could australia_could_have could_have_done have_done_great done_great_film great_film_about film_about_one about_one_australia one_australia_s australia_s_best s_best_moments best_moments_film moments_film_kokoda film_kokoda_shallow kokoda_shallow_disappointment\n",
      "Index: 8 | Predicted: positive | True: negative | Review: one line summary actually punch line old joke begins what jewish porno film while film had its interesting moments far slow moving did not_do enough explain those us audience unfamiliar orthodox jewish custom exactly what going why how many people who came across film would know bathtub female characters were washing reality called mikveh which ritual bath used cleanse spiritual uncleanliness same question might asked why bride walked around groom dizzying number times while her face covered prior marriage vows being performed these two examples two large number such moments remained completely unexplained uninitiated audience film does have its touching moments along expressions great love emotions characters presented authentically right down number garments ultra orthodox jewish male must wear well religious rituals he must engage upon awakening morning begin his day attitudes orthodox judaism has towards women general wives particular both intriguing times maddening another reason why more explanation needed story understood context i recommend film people who familiar orthodox jewish tradition ritual well those who might interested getting brief peek what lives people who practice way life like story itself about two sisters who own ways rebel against system moderate interest best one_line line_summary summary_actually actually_punch punch_line line_old old_joke joke_begins begins_what what_jewish jewish_porno porno_film film_while while_film film_had had_its its_interesting interesting_moments moments_far far_slow slow_moving moving_did did_not_do not_do_enough enough_explain explain_those those_us us_audience audience_unfamiliar unfamiliar_orthodox orthodox_jewish jewish_custom custom_exactly exactly_what what_going going_why why_how how_many many_people people_who who_came came_across across_film film_would would_know know_bathtub bathtub_female female_characters characters_were were_washing washing_reality reality_called called_mikveh mikveh_which which_ritual ritual_bath bath_used used_cleanse cleanse_spiritual spiritual_uncleanliness uncleanliness_same same_question question_might might_asked asked_why why_bride bride_walked walked_around around_groom groom_dizzying dizzying_number number_times times_while while_her her_face face_covered covered_prior prior_marriage marriage_vows vows_being being_performed performed_these these_two two_examples examples_two two_large large_number number_such such_moments moments_remained remained_completely completely_unexplained unexplained_uninitiated uninitiated_audience audience_film film_does does_have have_its its_touching touching_moments moments_along along_expressions expressions_great great_love love_emotions emotions_characters characters_presented presented_authentically authentically_right right_down down_number number_garments garments_ultra ultra_orthodox orthodox_jewish jewish_male male_must must_wear wear_well well_religious religious_rituals rituals_he he_must must_engage engage_upon upon_awakening awakening_morning morning_begin begin_his his_day day_attitudes attitudes_orthodox orthodox_judaism judaism_has has_towards towards_women women_general general_wives wives_particular particular_both both_intriguing intriguing_times times_maddening maddening_another another_reason reason_why why_more more_explanation explanation_needed needed_story story_understood understood_context context_i i_recommend recommend_film film_people people_who who_familiar familiar_orthodox orthodox_jewish jewish_tradition tradition_ritual ritual_well well_those those_who who_might might_interested interested_getting getting_brief brief_peek peek_what what_lives lives_people people_who who_practice practice_way way_life life_like like_story story_itself itself_about about_two two_sisters sisters_who who_own own_ways ways_rebel rebel_against against_system system_moderate moderate_interest interest_best one_line_summary line_summary_actually summary_actually_punch actually_punch_line punch_line_old line_old_joke old_joke_begins joke_begins_what begins_what_jewish what_jewish_porno jewish_porno_film porno_film_while film_while_film while_film_had film_had_its had_its_interesting its_interesting_moments interesting_moments_far moments_far_slow far_slow_moving slow_moving_did moving_did_not_do did_not_do_enough not_do_enough_explain enough_explain_those explain_those_us those_us_audience us_audience_unfamiliar audience_unfamiliar_orthodox unfamiliar_orthodox_jewish orthodox_jewish_custom jewish_custom_exactly custom_exactly_what exactly_what_going what_going_why going_why_how why_how_many how_many_people many_people_who people_who_came who_came_across came_across_film across_film_would film_would_know would_know_bathtub know_bathtub_female bathtub_female_characters female_characters_were characters_were_washing were_washing_reality washing_reality_called reality_called_mikveh called_mikveh_which mikveh_which_ritual which_ritual_bath ritual_bath_used bath_used_cleanse used_cleanse_spiritual cleanse_spiritual_uncleanliness spiritual_uncleanliness_same uncleanliness_same_question same_question_might question_might_asked might_asked_why asked_why_bride why_bride_walked bride_walked_around walked_around_groom around_groom_dizzying groom_dizzying_number dizzying_number_times number_times_while times_while_her while_her_face her_face_covered face_covered_prior covered_prior_marriage prior_marriage_vows marriage_vows_being vows_being_performed being_performed_these performed_these_two these_two_examples two_examples_two examples_two_large two_large_number large_number_such number_such_moments such_moments_remained moments_remained_completely remained_completely_unexplained completely_unexplained_uninitiated unexplained_uninitiated_audience uninitiated_audience_film audience_film_does film_does_have does_have_its have_its_touching its_touching_moments touching_moments_along moments_along_expressions along_expressions_great expressions_great_love great_love_emotions love_emotions_characters emotions_characters_presented characters_presented_authentically presented_authentically_right authentically_right_down right_down_number down_number_garments number_garments_ultra garments_ultra_orthodox ultra_orthodox_jewish orthodox_jewish_male jewish_male_must male_must_wear must_wear_well wear_well_religious well_religious_rituals religious_rituals_he rituals_he_must he_must_engage must_engage_upon engage_upon_awakening upon_awakening_morning awakening_morning_begin morning_begin_his begin_his_day his_day_attitudes day_attitudes_orthodox attitudes_orthodox_judaism orthodox_judaism_has judaism_has_towards has_towards_women towards_women_general women_general_wives general_wives_particular wives_particular_both particular_both_intriguing both_intriguing_times intriguing_times_maddening times_maddening_another maddening_another_reason another_reason_why reason_why_more why_more_explanation more_explanation_needed explanation_needed_story needed_story_understood story_understood_context understood_context_i context_i_recommend i_recommend_film recommend_film_people film_people_who people_who_familiar who_familiar_orthodox familiar_orthodox_jewish orthodox_jewish_tradition jewish_tradition_ritual tradition_ritual_well ritual_well_those well_those_who those_who_might who_might_interested might_interested_getting interested_getting_brief getting_brief_peek brief_peek_what peek_what_lives what_lives_people lives_people_who people_who_practice who_practice_way practice_way_life way_life_like life_like_story like_story_itself story_itself_about itself_about_two about_two_sisters two_sisters_who sisters_who_own who_own_ways own_ways_rebel ways_rebel_against rebel_against_system against_system_moderate system_moderate_interest moderate_interest_best\n",
      "Index: 9 | Predicted: negative | True: positive | Review: house dracula isn t all bad film rather decent times spoilers arriving home dr franz edelmann onslow stevens his seaside home count dracula john carradine discreetly seeks cure vampirism he starts work potential cure involving blood transfusion wolf man lawrence talbot lon chaney jr arrives his estate looking cure lycanthropy working two patients he discovers possible cure mold found near laboratory after searching area he finds frankenstein monster glenn strange buried nearby becoming obsessed reviving dr edelmann keeps neglecting dracula larry s requests after demanding get treatment instead him working monster turn each other climactic showdown good news rather decent film there one main idea quite creative imaginative first film openly propose idea vampirism blood disease one can transferred person person through exchange bodily fluid something would taken up later genre works rarely directly there s even microscope slide parasite believed responsible condition works some rather nicely used ideas comes across rather nifty idea even some execution little stale fact each creatures has least one standout scene nicely done idea wolf man has marvelous scene where he transforms inside prison cell doubting members search party goes crazy dracula s initial appearance appearing bat flying toward prone figure sleeping then appearing human form looks really impressive monster rampage well handled appropriate amount destruction caused large bat dracula transforms into always looks decent once quite realistically done s thoroughly decent affair bad news there several things weren t all great about one fact film combines much potentially intriguingly plots ideas really doesn t know what do them there several different back stories have mingled together which should clear enough mix well together seem coherent really doesn t have any plot rather flimsy doesn t really give preferential treatment any stars instead concentrates one then another then includes all three ending monsters only seem get engaged each other smallest possible reason makes big distraction ending once big let down seems entirely like changed last minute there s few other small things weren t all spectacular pretty much also contribute final verdict s quite decent film manages get through most time entertaining style nowhere near classic status each monsters debut features s nice enough watch fans monsters universal films general today s rating pg violence house_dracula dracula_isn isn_t t_all all_bad bad_film film_rather rather_decent decent_times times_spoilers spoilers_arriving arriving_home home_dr dr_franz franz_edelmann edelmann_onslow onslow_stevens stevens_his his_seaside seaside_home home_count count_dracula dracula_john john_carradine carradine_discreetly discreetly_seeks seeks_cure cure_vampirism vampirism_he he_starts starts_work work_potential potential_cure cure_involving involving_blood blood_transfusion transfusion_wolf wolf_man man_lawrence lawrence_talbot talbot_lon lon_chaney chaney_jr jr_arrives arrives_his his_estate estate_looking looking_cure cure_lycanthropy lycanthropy_working working_two two_patients patients_he he_discovers discovers_possible possible_cure cure_mold mold_found found_near near_laboratory laboratory_after after_searching searching_area area_he he_finds finds_frankenstein frankenstein_monster monster_glenn glenn_strange strange_buried buried_nearby nearby_becoming becoming_obsessed obsessed_reviving reviving_dr dr_edelmann edelmann_keeps keeps_neglecting neglecting_dracula dracula_larry larry_s s_requests requests_after after_demanding demanding_get get_treatment treatment_instead instead_him him_working working_monster monster_turn turn_each each_other other_climactic climactic_showdown showdown_good good_news news_rather rather_decent decent_film film_there there_one one_main main_idea idea_quite quite_creative creative_imaginative imaginative_first first_film film_openly openly_propose propose_idea idea_vampirism vampirism_blood blood_disease disease_one one_can can_transferred transferred_person person_person person_through through_exchange exchange_bodily bodily_fluid fluid_something something_would would_taken taken_up up_later later_genre genre_works works_rarely rarely_directly directly_there there_s s_even even_microscope microscope_slide slide_parasite parasite_believed believed_responsible responsible_condition condition_works works_some some_rather rather_nicely nicely_used used_ideas ideas_comes comes_across across_rather rather_nifty nifty_idea idea_even even_some some_execution execution_little little_stale stale_fact fact_each each_creatures creatures_has has_least least_one one_standout standout_scene scene_nicely nicely_done done_idea idea_wolf wolf_man man_has has_marvelous marvelous_scene scene_where where_he he_transforms transforms_inside inside_prison prison_cell cell_doubting doubting_members members_search search_party party_goes goes_crazy crazy_dracula dracula_s s_initial initial_appearance appearance_appearing appearing_bat bat_flying flying_toward toward_prone prone_figure figure_sleeping sleeping_then then_appearing appearing_human human_form form_looks looks_really really_impressive impressive_monster monster_rampage rampage_well well_handled handled_appropriate appropriate_amount amount_destruction destruction_caused caused_large large_bat bat_dracula dracula_transforms transforms_into into_always always_looks looks_decent decent_once once_quite quite_realistically realistically_done done_s s_thoroughly thoroughly_decent decent_affair affair_bad bad_news news_there there_several several_things things_weren weren_t t_all all_great great_about about_one one_fact fact_film film_combines combines_much much_potentially potentially_intriguingly intriguingly_plots plots_ideas ideas_really really_doesn doesn_t t_know know_what what_do do_them them_there there_several several_different different_back back_stories stories_have have_mingled mingled_together together_which which_should should_clear clear_enough enough_mix mix_well well_together together_seem seem_coherent coherent_really really_doesn doesn_t t_have have_any any_plot plot_rather rather_flimsy flimsy_doesn doesn_t t_really really_give give_preferential preferential_treatment treatment_any any_stars stars_instead instead_concentrates concentrates_one one_then then_another another_then then_includes includes_all all_three three_ending ending_monsters monsters_only only_seem seem_get get_engaged engaged_each each_other other_smallest smallest_possible possible_reason reason_makes makes_big big_distraction distraction_ending ending_once once_big big_let let_down down_seems seems_entirely entirely_like like_changed changed_last last_minute minute_there there_s s_few few_other other_small small_things things_weren weren_t t_all all_spectacular spectacular_pretty pretty_much much_also also_contribute contribute_final final_verdict verdict_s s_quite quite_decent decent_film film_manages manages_get get_through through_most most_time time_entertaining entertaining_style style_nowhere nowhere_near near_classic classic_status status_each each_monsters monsters_debut debut_features features_s s_nice nice_enough enough_watch watch_fans fans_monsters monsters_universal universal_films films_general general_today today_s s_rating rating_pg pg_violence house_dracula_isn dracula_isn_t isn_t_all t_all_bad all_bad_film bad_film_rather film_rather_decent rather_decent_times decent_times_spoilers times_spoilers_arriving spoilers_arriving_home arriving_home_dr home_dr_franz dr_franz_edelmann franz_edelmann_onslow edelmann_onslow_stevens onslow_stevens_his stevens_his_seaside his_seaside_home seaside_home_count home_count_dracula count_dracula_john dracula_john_carradine john_carradine_discreetly carradine_discreetly_seeks discreetly_seeks_cure seeks_cure_vampirism cure_vampirism_he vampirism_he_starts he_starts_work starts_work_potential work_potential_cure potential_cure_involving cure_involving_blood involving_blood_transfusion blood_transfusion_wolf transfusion_wolf_man wolf_man_lawrence man_lawrence_talbot lawrence_talbot_lon talbot_lon_chaney lon_chaney_jr chaney_jr_arrives jr_arrives_his arrives_his_estate his_estate_looking estate_looking_cure looking_cure_lycanthropy cure_lycanthropy_working lycanthropy_working_two working_two_patients two_patients_he patients_he_discovers he_discovers_possible discovers_possible_cure possible_cure_mold cure_mold_found mold_found_near found_near_laboratory near_laboratory_after laboratory_after_searching after_searching_area searching_area_he area_he_finds he_finds_frankenstein finds_frankenstein_monster frankenstein_monster_glenn monster_glenn_strange glenn_strange_buried strange_buried_nearby buried_nearby_becoming nearby_becoming_obsessed becoming_obsessed_reviving obsessed_reviving_dr reviving_dr_edelmann dr_edelmann_keeps edelmann_keeps_neglecting keeps_neglecting_dracula neglecting_dracula_larry dracula_larry_s larry_s_requests s_requests_after requests_after_demanding after_demanding_get demanding_get_treatment get_treatment_instead treatment_instead_him instead_him_working him_working_monster working_monster_turn monster_turn_each turn_each_other each_other_climactic other_climactic_showdown climactic_showdown_good showdown_good_news good_news_rather news_rather_decent rather_decent_film decent_film_there film_there_one there_one_main one_main_idea main_idea_quite idea_quite_creative quite_creative_imaginative creative_imaginative_first imaginative_first_film first_film_openly film_openly_propose openly_propose_idea propose_idea_vampirism idea_vampirism_blood vampirism_blood_disease blood_disease_one disease_one_can one_can_transferred can_transferred_person transferred_person_person person_person_through person_through_exchange through_exchange_bodily exchange_bodily_fluid bodily_fluid_something fluid_something_would something_would_taken would_taken_up taken_up_later up_later_genre later_genre_works genre_works_rarely works_rarely_directly rarely_directly_there directly_there_s there_s_even s_even_microscope even_microscope_slide microscope_slide_parasite slide_parasite_believed parasite_believed_responsible believed_responsible_condition responsible_condition_works condition_works_some works_some_rather some_rather_nicely rather_nicely_used nicely_used_ideas used_ideas_comes ideas_comes_across comes_across_rather across_rather_nifty rather_nifty_idea nifty_idea_even idea_even_some even_some_execution some_execution_little execution_little_stale little_stale_fact stale_fact_each fact_each_creatures each_creatures_has creatures_has_least has_least_one least_one_standout one_standout_scene standout_scene_nicely scene_nicely_done nicely_done_idea done_idea_wolf idea_wolf_man wolf_man_has man_has_marvelous has_marvelous_scene marvelous_scene_where scene_where_he where_he_transforms he_transforms_inside transforms_inside_prison inside_prison_cell prison_cell_doubting cell_doubting_members doubting_members_search members_search_party search_party_goes party_goes_crazy goes_crazy_dracula crazy_dracula_s dracula_s_initial s_initial_appearance initial_appearance_appearing appearance_appearing_bat appearing_bat_flying bat_flying_toward flying_toward_prone toward_prone_figure prone_figure_sleeping figure_sleeping_then sleeping_then_appearing then_appearing_human appearing_human_form human_form_looks form_looks_really looks_really_impressive really_impressive_monster impressive_monster_rampage monster_rampage_well rampage_well_handled well_handled_appropriate handled_appropriate_amount appropriate_amount_destruction amount_destruction_caused destruction_caused_large caused_large_bat large_bat_dracula bat_dracula_transforms dracula_transforms_into transforms_into_always into_always_looks always_looks_decent looks_decent_once decent_once_quite once_quite_realistically quite_realistically_done realistically_done_s done_s_thoroughly s_thoroughly_decent thoroughly_decent_affair decent_affair_bad affair_bad_news bad_news_there news_there_several there_several_things several_things_weren things_weren_t weren_t_all t_all_great all_great_about great_about_one about_one_fact one_fact_film fact_film_combines film_combines_much combines_much_potentially much_potentially_intriguingly potentially_intriguingly_plots intriguingly_plots_ideas plots_ideas_really ideas_really_doesn really_doesn_t doesn_t_know t_know_what know_what_do what_do_them do_them_there them_there_several there_several_different several_different_back different_back_stories back_stories_have stories_have_mingled have_mingled_together mingled_together_which together_which_should which_should_clear should_clear_enough clear_enough_mix enough_mix_well mix_well_together well_together_seem together_seem_coherent seem_coherent_really coherent_really_doesn really_doesn_t doesn_t_have t_have_any have_any_plot any_plot_rather plot_rather_flimsy rather_flimsy_doesn flimsy_doesn_t doesn_t_really t_really_give really_give_preferential give_preferential_treatment preferential_treatment_any treatment_any_stars any_stars_instead stars_instead_concentrates instead_concentrates_one concentrates_one_then one_then_another then_another_then another_then_includes then_includes_all includes_all_three all_three_ending three_ending_monsters ending_monsters_only monsters_only_seem only_seem_get seem_get_engaged get_engaged_each engaged_each_other each_other_smallest other_smallest_possible smallest_possible_reason possible_reason_makes reason_makes_big makes_big_distraction big_distraction_ending distraction_ending_once ending_once_big once_big_let big_let_down let_down_seems down_seems_entirely seems_entirely_like entirely_like_changed like_changed_last changed_last_minute last_minute_there minute_there_s there_s_few s_few_other few_other_small other_small_things small_things_weren things_weren_t weren_t_all t_all_spectacular all_spectacular_pretty spectacular_pretty_much pretty_much_also much_also_contribute also_contribute_final contribute_final_verdict final_verdict_s verdict_s_quite s_quite_decent quite_decent_film decent_film_manages film_manages_get manages_get_through get_through_most through_most_time most_time_entertaining time_entertaining_style entertaining_style_nowhere style_nowhere_near nowhere_near_classic near_classic_status classic_status_each status_each_monsters each_monsters_debut monsters_debut_features debut_features_s features_s_nice s_nice_enough nice_enough_watch enough_watch_fans watch_fans_monsters fans_monsters_universal monsters_universal_films universal_films_general films_general_today general_today_s today_s_rating s_rating_pg rating_pg_violence\n",
      "Index: 11 | Predicted: positive | True: negative | Review: film heaven s gate good view although still tedious over 4 hours film took great license usual hollywood james averill chris christopherson elle were actually married real life main contribution johnson county war start being hanged well starting i mean came beginning not_the end here s real scenario james averill ellen watson were secretly married because one homestead could given each family filing single individuals could get two homesteads chose homesites crazy woman creek actually controlling water above land held powerful member cattleman s association he offered buy them out repeatedly which refused although characterized real life owner brothel cattle kate prostitute herself also film there no real evidence true known she bought many head sick cattle nursed them back life later accused cattleman s association receiving cattle trade lewd acts end she accused rustling act almost certainly untrue much part myth american west which gooble de gook myths spanning time period about one hundred years real life she jim averill were surprised one day several members cattleman s association taken hand promptly hanged those perpetrating injustice were never brought trial first link led murder nate champion start johnson county war quite different hollywood version which shows her shot end other than i think main problem film editor who could have made action faster pace more skillful editing film_heaven heaven_s s_gate gate_good good_view view_although although_still still_tedious tedious_over over_4 4_hours hours_film film_took took_great great_license license_usual usual_hollywood hollywood_james james_averill averill_chris chris_christopherson christopherson_elle elle_were were_actually actually_married married_real real_life life_main main_contribution contribution_johnson johnson_county county_war war_start start_being being_hanged hanged_well well_starting starting_i i_mean mean_came came_beginning beginning_not_the not_the_end end_here here_s s_real real_scenario scenario_james james_averill averill_ellen ellen_watson watson_were were_secretly secretly_married married_because because_one one_homestead homestead_could could_given given_each each_family family_filing filing_single single_individuals individuals_could could_get get_two two_homesteads homesteads_chose chose_homesites homesites_crazy crazy_woman woman_creek creek_actually actually_controlling controlling_water water_above above_land land_held held_powerful powerful_member member_cattleman cattleman_s s_association association_he he_offered offered_buy buy_them them_out out_repeatedly repeatedly_which which_refused refused_although although_characterized characterized_real real_life life_owner owner_brothel brothel_cattle cattle_kate kate_prostitute prostitute_herself herself_also also_film film_there there_no no_real real_evidence evidence_true true_known known_she she_bought bought_many many_head head_sick sick_cattle cattle_nursed nursed_them them_back back_life life_later later_accused accused_cattleman cattleman_s s_association association_receiving receiving_cattle cattle_trade trade_lewd lewd_acts acts_end end_she she_accused accused_rustling rustling_act act_almost almost_certainly certainly_untrue untrue_much much_part part_myth myth_american american_west west_which which_gooble gooble_de de_gook gook_myths myths_spanning spanning_time time_period period_about about_one one_hundred hundred_years years_real real_life life_she she_jim jim_averill averill_were were_surprised surprised_one one_day day_several several_members members_cattleman cattleman_s s_association association_taken taken_hand hand_promptly promptly_hanged hanged_those those_perpetrating perpetrating_injustice injustice_were were_never never_brought brought_trial trial_first first_link link_led led_murder murder_nate nate_champion champion_start start_johnson johnson_county county_war war_quite quite_different different_hollywood hollywood_version version_which which_shows shows_her her_shot shot_end end_other other_than than_i i_think think_main main_problem problem_film film_editor editor_who who_could could_have have_made made_action action_faster faster_pace pace_more more_skillful skillful_editing film_heaven_s heaven_s_gate s_gate_good gate_good_view good_view_although view_although_still although_still_tedious still_tedious_over tedious_over_4 over_4_hours 4_hours_film hours_film_took film_took_great took_great_license great_license_usual license_usual_hollywood usual_hollywood_james hollywood_james_averill james_averill_chris averill_chris_christopherson chris_christopherson_elle christopherson_elle_were elle_were_actually were_actually_married actually_married_real married_real_life real_life_main life_main_contribution main_contribution_johnson contribution_johnson_county johnson_county_war county_war_start war_start_being start_being_hanged being_hanged_well hanged_well_starting well_starting_i starting_i_mean i_mean_came mean_came_beginning came_beginning_not_the beginning_not_the_end not_the_end_here end_here_s here_s_real s_real_scenario real_scenario_james scenario_james_averill james_averill_ellen averill_ellen_watson ellen_watson_were watson_were_secretly were_secretly_married secretly_married_because married_because_one because_one_homestead one_homestead_could homestead_could_given could_given_each given_each_family each_family_filing family_filing_single filing_single_individuals single_individuals_could individuals_could_get could_get_two get_two_homesteads two_homesteads_chose homesteads_chose_homesites chose_homesites_crazy homesites_crazy_woman crazy_woman_creek woman_creek_actually creek_actually_controlling actually_controlling_water controlling_water_above water_above_land above_land_held land_held_powerful held_powerful_member powerful_member_cattleman member_cattleman_s cattleman_s_association s_association_he association_he_offered he_offered_buy offered_buy_them buy_them_out them_out_repeatedly out_repeatedly_which repeatedly_which_refused which_refused_although refused_although_characterized although_characterized_real characterized_real_life real_life_owner life_owner_brothel owner_brothel_cattle brothel_cattle_kate cattle_kate_prostitute kate_prostitute_herself prostitute_herself_also herself_also_film also_film_there film_there_no there_no_real no_real_evidence real_evidence_true evidence_true_known true_known_she known_she_bought she_bought_many bought_many_head many_head_sick head_sick_cattle sick_cattle_nursed cattle_nursed_them nursed_them_back them_back_life back_life_later life_later_accused later_accused_cattleman accused_cattleman_s cattleman_s_association s_association_receiving association_receiving_cattle receiving_cattle_trade cattle_trade_lewd trade_lewd_acts lewd_acts_end acts_end_she end_she_accused she_accused_rustling accused_rustling_act rustling_act_almost act_almost_certainly almost_certainly_untrue certainly_untrue_much untrue_much_part much_part_myth part_myth_american myth_american_west american_west_which west_which_gooble which_gooble_de gooble_de_gook de_gook_myths gook_myths_spanning myths_spanning_time spanning_time_period time_period_about period_about_one about_one_hundred one_hundred_years hundred_years_real years_real_life real_life_she life_she_jim she_jim_averill jim_averill_were averill_were_surprised were_surprised_one surprised_one_day one_day_several day_several_members several_members_cattleman members_cattleman_s cattleman_s_association s_association_taken association_taken_hand taken_hand_promptly hand_promptly_hanged promptly_hanged_those hanged_those_perpetrating those_perpetrating_injustice perpetrating_injustice_were injustice_were_never were_never_brought never_brought_trial brought_trial_first trial_first_link first_link_led link_led_murder led_murder_nate murder_nate_champion nate_champion_start champion_start_johnson start_johnson_county johnson_county_war county_war_quite war_quite_different quite_different_hollywood different_hollywood_version hollywood_version_which version_which_shows which_shows_her shows_her_shot her_shot_end shot_end_other end_other_than other_than_i than_i_think i_think_main think_main_problem main_problem_film problem_film_editor film_editor_who editor_who_could who_could_have could_have_made have_made_action made_action_faster action_faster_pace faster_pace_more pace_more_skillful more_skillful_editing\n",
      "\n",
      "New Reviews Predictions:\n",
      "Review: I liked this movie. It was very enetertaining.\n",
      "Predicted Sentiment: positive\n",
      "Review: I didn't enjoy this movie, it was terrible. The plot made no sense and the acting was wooden.\n",
      "Predicted Sentiment: negative\n",
      "\n",
      "TF-IDF min_doc_freq_percent = 0.05 -> Test Accuracy: 0.8003520140805632\n",
      "\n",
      "TF-IDF min_doc_freq_percent = 0.1 -> Test Accuracy: 0.7372694907796312\n"
     ]
    }
   ],
   "source": [
    "from Tokenizer import preprocess_text\n",
    "# preprocess_text is defined in Tokenizer.py\n",
    "\n",
    "# Find 5 misclassified reviews\n",
    "misclassified = []\n",
    "for i, (pred, true) in enumerate(zip(test_preds_final, test_labels)):\n",
    "    if pred != true:\n",
    "        misclassified.append((i, pred, true, test_data[i]))\n",
    "    if len(misclassified) >= 5:\n",
    "        break\n",
    "\n",
    "print(\"5 Misclassified Reviews (index, predicted, true, review):\")\n",
    "for idx, pred, true, tokens in misclassified:\n",
    "    print(\"Index:\", idx, \"| Predicted:\", pred, \"| True:\", true, \"| Review:\", \" \".join(tokens))\n",
    "\n",
    "# Testing  two new movie reviews\n",
    "new_reviews = [\n",
    "    \"I liked this movie. It was very enetertaining.\",\n",
    "    \"I didn't enjoy this movie, it was terrible. The plot made no sense and the acting was wooden.\"\n",
    "]\n",
    "\n",
    "# Preprocess and compute TF-IDF for the new reviews\n",
    "new_reviews_tokens = [preprocess_text(review, vocab=vocab) for review in new_reviews]\n",
    "new_reviews_tfidf = [calcTfidfval(tokens, selected_tokens, idf) for tokens in new_reviews_tokens]\n",
    "\n",
    "print(\"\\nNew Reviews Predictions:\")\n",
    "for review, tfidf in zip(new_reviews, new_reviews_tfidf):\n",
    "    pred = clf_final(tfidf)\n",
    "    print(\"Review:\", review)\n",
    "    print(\"Predicted Sentiment:\", pred)\n",
    "\n",
    "# Comparinhg performance with different TF-IDF thresholds\n",
    "def calcTfidfval_features(min_doc_freq_percent):\n",
    "    num_docs = len(train_data)\n",
    "    min_count = math.ceil(min_doc_freq_percent * num_docs)\n",
    "    df_counter = Counter()\n",
    "    for review in train_data:\n",
    "        df_counter.update(set(review))\n",
    "    sel_tokens_new = {token for token, count in df_counter.items() if count >= min_count}\n",
    "    idf_new = {}\n",
    "    for token in sel_tokens_new:\n",
    "        idf_new[token] = math.log((num_docs + 1) / (df_counter[token] + 1)) + 1\n",
    "    train_tfidf_new = [calcTfidfval(review, sel_tokens_new, idf_new) for review in train_data]\n",
    "    test_tfidf_new = [calcTfidfval(review, sel_tokens_new, idf_new) for review in test_data]\n",
    "    return sel_tokens_new, idf_new, train_tfidf_new, test_tfidf_new\n",
    "\n",
    "for thresh in [0.05, 0.1]:\n",
    "    sel_tokens_new, idf_new, train_tfidf_new, test_tfidf_new = calcTfidfval_features(thresh)\n",
    "    train_tfidf_us_new, train_labels_us_new = undersample(train_tfidf_new, train_labels)\n",
    "    clf_new = train_bayes_classifier(train_tfidf_us_new, train_labels_us_new)\n",
    "    test_preds_new = [clf_new(x) for x in test_tfidf_new]\n",
    "    acc_new = sum(1 for p, t in zip(test_preds_new, test_labels) if p == t) / len(test_labels)\n",
    "    print(\"\\nTF-IDF min_doc_freq_percent =\", thresh, \"-> Test Accuracy:\", acc_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The proposed work first evaluated the predicted sentiment labels against the true actual labels in the test data set. If the prediction did not match the true label, then the corresponding review was marked as misclassified. This process was stopped after identifying 5 misclassified reviews for manual reviews. These misclassified reviews were printed along with their true and predicted sentiment labels. They were analyzed to detect any patterns in the errors (e.g. sarcasm, ambiguous language or unseen vocabulary). Two new movie reviews were manually written one positive, and one negative, and were processed using same TF-IDF feature extraction as the test data. The trained classifier predicted the sentiment for these new reviews. The model’s performance was analyzed under different TF-IDF thresholds for minimum document frequency (0.05 and 0.1). For each threshold new feature sets were calculated, and a classifier was trained using the under sampled data. A lower TF-IDF threshold (0.05) retained more words and achieved a test accuracy of 80.03 %. This captured finer details and potentially more noise too, leading to overfitting.  The higher threshold model (0.1) achieved 73.72 % test accuracy. Fewer features in this case led to loss of information. This improved generalization but decreased accuracy. With manual evaluation the study found out where the models could be improved such as handling sarcasm, negation, long tailed words, and domain specific references."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 7 : Implement an Improved Feature Selection Method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens before filtering: 5251541\n",
      "Tokens after filtering: 434\n",
      "Improved Model Test Accuracy: 0.8003520140805632\n"
     ]
    }
   ],
   "source": [
    "# Improved feature selection by removing highly frequently occuring words\n",
    "\n",
    "# defing the minimum and maximum percentage of doc a word should appear in\n",
    "min_percent = 0.05  # min 5 %\n",
    "max_percent = 0.8   # max 80 %\n",
    "\n",
    "# calculating the minimum and maximum number of doc a word should appear in\n",
    "total_docs = len(train_data)\n",
    "min_count = math.ceil(min_percent * total_docs)\n",
    "max_count = math.floor(max_percent * total_docs)\n",
    "\n",
    "\n",
    "word_doc_count = Counter()\n",
    "for review in train_data:\n",
    "    word_doc_count.update(set(review))  \n",
    "\n",
    "# Filter out words that don't meet the doc frequency threshold\n",
    "filtered_words = {word for word, count in word_doc_count.items() if min_count <= count <= max_count}\n",
    "\n",
    "print(\"Tokens before filtering:\", len(word_doc_count))\n",
    "print(\"Tokens after filtering:\", len(filtered_words))\n",
    "\n",
    "# Calculating the IDF \n",
    "idf_values = {}\n",
    "for word in filtered_words:\n",
    "    doc_count = word_doc_count[word]\n",
    "    idf_values[word] = math.log((total_docs + 1) / (doc_count + 1)) + 1\n",
    "\n",
    "\n",
    "train_tfidf_filtered = [calcTfidfval(review, filtered_words, idf_values) for review in train_data]\n",
    "test_tfidf_filtered = [calcTfidfval(review, filtered_words, idf_values) for review in test_data]\n",
    "\n",
    "# Training the classifier with the filtered data\n",
    "train_tfidf_filtered_us, train_labels_filtered_us = undersample(train_tfidf_filtered, train_labels)\n",
    "clf_filtered = train_bayes_classifier(train_tfidf_filtered_us, train_labels_filtered_us)\n",
    "test_preds_filtered = [clf_filtered(x) for x in test_tfidf_filtered]\n",
    "accuracy_filtered = sum(1 for p, t in zip(test_preds_filtered, test_labels) if p == t) / len(test_labels)\n",
    "\n",
    "print(\"Improved Model Test Accuracy:\", accuracy_filtered)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
